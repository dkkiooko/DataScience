{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072b2595",
   "metadata": {},
   "source": [
    "<h4> Import libraries to use </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79296fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631dbd5",
   "metadata": {},
   "source": [
    "<h4> methods to find impurity of a node </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cee66f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"_finds the gini impurity of a node. Measure of impurity in the system_\n",
    "    Args:\n",
    "        y (_Pandas Series_): _all our class labels_\n",
    "    Returns:\n",
    "        _float_: _entropy ranges from 0(purest) to 1(most impure)._\n",
    "    \"\"\"\n",
    "    if isinstance(y, pd.Series):\n",
    "        p = y.value_counts()/y.shape[0]\n",
    "        gini = 1-np.sum(p**2)\n",
    "        return(gini)\n",
    "    else:\n",
    "        raise('Object must be a Pandas Series.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93dd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"_finds the entropy of a node. Measure of impurity in the system_\n",
    "\n",
    "    Args:\n",
    "        y (_Pandas Series_): _all our class labels_\n",
    "\n",
    "    Returns:\n",
    "        _float_: _gini index ranges from 0(purest) to 1(most impure)._\n",
    "    \"\"\"    \n",
    "    if isinstance(y, pd.Series):\n",
    "        a = y.value_counts()/y.shape[0]\n",
    "        entropy = np.sum(-a*np.log2(a+1e-9))\n",
    "        return(entropy)\n",
    "    else:\n",
    "        raise('Object must be a Pandas Series.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890647c",
   "metadata": {},
   "source": [
    "<h4> methods to choose cuts for decision tree </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3ee0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(y):\n",
    "    \"\"\"_finds variance if column is numerical_\n",
    "\n",
    "    Args:\n",
    "        y (_Pandas Series_): _column with predictors_\n",
    "\n",
    "    Returns:\n",
    "        _float_: _variance of the column._\n",
    "    \"\"\"\n",
    "    if(len(y) == 1):\n",
    "        return 0\n",
    "    else:\n",
    "        return y.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf94420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, filter_column, func=entropy):\n",
    "    \"\"\"_get information gain from parent to child nodes\n",
    "    \n",
    "    Args:\n",
    "        y (_Pandas Series_): _labels in current node_\n",
    "        filter_column (_Pandas Series_): subset of training data on which to split node\n",
    "        func (_function_): function to find impurity of node\n",
    "        \n",
    "    Returns:\n",
    "        _float_: _this is the information gain. between 0 and 1\n",
    "    \n",
    "    \"\"\"\n",
    "    a = sum(filter_column)\n",
    "    b = filter_column.shape[0] - a\n",
    "  \n",
    "    if(a == 0 or b ==0):\n",
    "        # if either is 0, it means only 1 child has been created that is a replica of the parent\n",
    "        # thus there is no information gain\n",
    "        information_gain = 0\n",
    "  \n",
    "    else:\n",
    "        if y.dtypes != 'O':\n",
    "            # numerical columns\n",
    "            information_gain = variance(y) - (a/(a+b)* variance(y[filter_column])) - (b/(a+b)*variance(y[-filter_column]))\n",
    "        else:\n",
    "            # categorical columns\n",
    "            information_gain = func(y)-a/(a+b)*func(y[filter_column])-b/(a+b)*func(y[-filter_column])\n",
    "  \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120549c",
   "metadata": {},
   "source": [
    "<h4> methods to calculate the best split for a variable </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf6e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_options(x_column):\n",
    "    \"\"\"_find all the possible combinations in categorical column\n",
    "   this is for the random aspect during the calculation of best split\n",
    "   \n",
    "   Args:\n",
    "       x_column (_Pandas Series_): categorical column on which to find subsets\n",
    "    \n",
    "    Returns:\n",
    "        options (_list_): list of lists with combinations\n",
    "   \n",
    "   \"\"\"\n",
    "    x_column = x_column.unique()\n",
    "\n",
    "    options = []\n",
    "    for L in range(0, len(x_column)+1):\n",
    "        for subset in itertools.combinations(x_column, L):\n",
    "            subset = list(subset)\n",
    "            options.append(subset)\n",
    "    # first element in options is empty, last is not subset\n",
    "    return options[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de705bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_information_gain_split(x, y, func=entropy):\n",
    "    \"\"\"_find the max information gain split\n",
    "    \n",
    "    Args:\n",
    "        x (_Pandas Series): _predictor variable_\n",
    "        y (_Pandas Series): _target variable_\n",
    "        func: function to be used for the best split\n",
    "    \n",
    "    Return:\n",
    "        tuple: best_split(value), information gain for that split, type of variable(numeric/categorical), whether or not there was information gain\n",
    "    \"\"\"\n",
    "    split_value = []\n",
    "    information_gains = [] \n",
    "\n",
    "    numeric_variable = True if x.dtypes != 'O' else False\n",
    "\n",
    "  # Create options according to variable type\n",
    "    if numeric_variable:\n",
    "        options = x.sort_values().unique()[1:]\n",
    "    else: \n",
    "        options = categorical_options(x)\n",
    "\n",
    "    # Calculate ig for all values\n",
    "    for val in options:\n",
    "        filter_column =   x < val if numeric_variable else x.isin(val)\n",
    "        val_ig = information_gain(y, filter_column, func)\n",
    "        # Append results\n",
    "        information_gains.append(val_ig)\n",
    "        split_value.append(val)\n",
    "\n",
    "  # Check if there are more than 1 results if not, return False\n",
    "    if len(information_gains) == 0:\n",
    "        return(None,None,None, False)\n",
    "\n",
    "    else:\n",
    "  # Get results with highest IG\n",
    "        best_ig = max(information_gains)\n",
    "        best_ig_index = information_gains.index(best_ig)\n",
    "        best_split = split_value[best_ig_index]\n",
    "        return(best_ig,best_split,numeric_variable, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90a4c8",
   "metadata": {},
   "source": [
    "<h4> methods to get best split, make the split and make a prediction </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad23622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(name, data):\n",
    "    \"\"\"_get the best split of a node_\n",
    "\n",
    "    Args:\n",
    "        y (_string_): _name of target variable_\n",
    "        data (_Pandas DataFrame_): _training data_\n",
    "\n",
    "    Returns:\n",
    "        _tuple_: _variable with highest information gain, value on which to split variable,\n",
    "                    value of the information gain, whether variable is numerical or categorical_\n",
    "    \"\"\"    \n",
    "    best_split_df = data.drop(name, axis= 1).apply(max_information_gain_split, y = data[name])\n",
    "    if sum(best_split_df.loc[3,:]) == 0:\n",
    "        return(None, None, None, None)\n",
    "\n",
    "    else:\n",
    "        # Get only masks that can be splitted\n",
    "        best_split_df = best_split_df.loc[:,best_split_df.loc[3,:]]\n",
    "\n",
    "        # Get the results for split with highest Information Gain\n",
    "        split_variable = best_split_df.iloc[0].astype(np.float32).idxmax()\n",
    "        split_value = best_split_df[split_variable][1] \n",
    "        split_information_gain = best_split_df[split_variable][0]\n",
    "        split_numeric = best_split_df[split_variable][2]\n",
    "\n",
    "        return(split_variable, split_value, split_information_gain, split_numeric)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "087f10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(variable, value, data, is_numeric):\n",
    "    \"\"\"_create 2 child nodes from parent node_\n",
    "\n",
    "    Args:\n",
    "        variable (_string_): _name of variable_\n",
    "        data (_Pandas DataFrame_): _training data to be splitted_\n",
    "        value (_int or float or string_): _value of variable to make the split_\n",
    "        is_numeric (_bool_): is variable to be splitted numeric or not\n",
    "\n",
    "    Returns:\n",
    "        _tuple_: _2 child nodes, left node and right node_\n",
    "    \"\"\" \n",
    "    if is_numeric:\n",
    "        left_node = data[data[variable] < value]\n",
    "        right_node = data[(data[variable] < value) == False]\n",
    "\n",
    "    else:\n",
    "        left_node = data[data[variable].isin(value)]\n",
    "        right_node = data[(data[variable].isin(value)) == False]\n",
    "\n",
    "    return(left_node, right_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4b2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(data, is_numeric):\n",
    "    \"\"\"_make prediction._\n",
    "\n",
    "    Args:\n",
    "        data (_Pandas DataFrame_): _training data to be splitted_\n",
    "        is_numeric (_bool_): is variable numeric or categorical\n",
    "\n",
    "    Returns:\n",
    "        _prediction_: _prediction for tree_\n",
    "    \"\"\" \n",
    "    if is_numeric:\n",
    "        pred = data.value_counts().idxmax()\n",
    "    else:\n",
    "        pred = data.mean()\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56bb04",
   "metadata": {},
   "source": [
    "<h4> train our decision tree </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beedf8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree(data,y, is_numeric, max_depth = None,min_samples_split = None, min_information_gain = 1e-20, counter=0, max_categories = 20):\n",
    "    \"\"\"_train a Decision Tree_\n",
    "\n",
    "    Args:\n",
    "        data (_Pandas DataFrame_): _training data for the Decision Tree_\n",
    "        y (_string_): _name of target varible_\n",
    "        is_numeric (_bool_): is target variable categorical or numeric\n",
    "        max_depth (_int_): depth on which to stop splitting\n",
    "        min_samples_split (_int_): number of samples below which a new child node cant be created\n",
    "        min_information_gain (_float_): smallest amount of information gain for a split to be considered valid\n",
    "        max_categories (_int_): max number of categories in a categorical column. Many unique items cause high computation time and slow learning\n",
    "        counter (int): keeps count of depth which is also the number of recursions\n",
    "        \n",
    "    Returns:\n",
    "        _subtree_: _this is a set of rules used to predict_\n",
    "    \"\"\"    \n",
    "    if counter==0:\n",
    "        # on first iteration, check each categorical column\n",
    "        # if any column has more unique values than max_categories, raise error\n",
    "        types = data.dtypes\n",
    "        # categorical columns\n",
    "        check_columns = types[types == \"object\"].index\n",
    "        for column in check_columns:\n",
    "            # number of unique values in column\n",
    "            var_length = len(data[column].value_counts()) \n",
    "            if var_length > max_categories:\n",
    "                raise ValueError('The variable ' + column + ' has '+ str(var_length) + ' unique values, which is more than the accepted ones: ' +  str(max_categories))\n",
    "  \n",
    "    # Check for depth conditions on every recursion iteration\n",
    "    if max_depth == None:\n",
    "        depth_cond = True\n",
    "    else:\n",
    "        if counter < max_depth:\n",
    "            depth_cond = True\n",
    "        else:\n",
    "            depth_cond = False\n",
    "    # Check for sample conditions on every recursion iteration\n",
    "    if min_samples_split == None:\n",
    "        sample_cond = True\n",
    "    else:\n",
    "        if data.shape[0] > min_samples_split:\n",
    "            sample_cond = True\n",
    "        else:\n",
    "            sample_cond = False\n",
    "    # if stopping conditions are not broken\n",
    "    if depth_cond & sample_cond:\n",
    "        split_variable, split_value, split_information_gain, split_numeric = get_best_split(y, data)\n",
    "    # If ig condition is fulfilled, make split \n",
    "        if split_information_gain is not None and split_information_gain >= min_information_gain:\n",
    "            counter += 1  # on every\n",
    "            left,right = make_split(split_variable, split_value, data, split_numeric)\n",
    "      # document the tree\n",
    "            split_type = \"<=\" if split_numeric else \"in\"\n",
    "            question =   \"{} {}  {}\".format(split_variable,split_type,split_value)\n",
    "            subtree = {question: []}\n",
    "      # Find answers (recursion)\n",
    "            yes_answer = train_tree(left,y, target_type, max_depth,min_samples_split,min_information_gain, counter)\n",
    "            no_answer = train_tree(right,y, target_type, max_depth,min_samples_split,min_information_gain, counter)\n",
    "            if yes_answer == no_answer: # will only happen at leaf node\n",
    "                subtree = yes_answer\n",
    "            else:\n",
    "                subtree[question].append(yes_answer)\n",
    "                subtree[question].append(no_answer)\n",
    "    # If it doesn't match IG condition, make prediction\n",
    "        else:\n",
    "            pred = make_prediction(data[y],is_numeric)\n",
    "            return pred\n",
    "   # Drop dataset if doesn't match depth or sample conditions\n",
    "    else:\n",
    "        pred = make_prediction(data[y],is_numeric)\n",
    "        return pred\n",
    "    return subtree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80e3da",
   "metadata": {},
   "source": [
    "<h4> method to make prediction </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f46f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_regress(datapoint, subtree):\n",
    "    \"\"\"_makes predictions, classification or regression_\n",
    "    \n",
    "    Args:\n",
    "        datapoint (_numpy row_): single instance to be predicted\n",
    "        subtree (_Decision Tree_): trained tree\n",
    "        \n",
    "    Returns:\n",
    "        answer (_type_): a prediction for the datapoint\n",
    "    \"\"\"\n",
    "    # get subtree of decisions\n",
    "    question = list(subtree.keys())[0]\n",
    "    \n",
    "    # check whether variable type is numerical or categorical\n",
    "    if question.split()[1] == '<=':\n",
    "        if datapoint[question.split()[0]] <= float(question.split()[2]):\n",
    "            answer = subtree[question][0]\n",
    "        else:\n",
    "            answer = subtree[question][1]\n",
    "    else:\n",
    "        if datapoint[question.split()[0]] in (question.split()[2]):\n",
    "            answer = subtree[question][0]\n",
    "        else:\n",
    "            answer = subtree[question][1]\n",
    "    \n",
    "    # if the answer is not a dictionary means we've reached a leaf node\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "        return classify_regress(datapoint, residual_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a173c",
   "metadata": {},
   "source": [
    "<h4> Predict sample dataset with decision tree </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f68b55b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20840\\3775802651.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitanic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'titanic.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bed573",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_lite = titanic.loc[:,[\"Embarked\", \"Age\", \"Fare\", \"Survived\"]]\n",
    "titanic_lite = titanic_lite.loc[titanic_lite.isna().sum(axis=1) == 0, :]\n",
    "titanic_tree = train_tree(titanic_lite, 'Survived', True, max_depth=10, min_samples_split=10)\n",
    "titanic_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b84594",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_prediction = []\n",
    "num_obs = 20\n",
    "\n",
    "for i in range(num_obs):\n",
    "    obs_pred = classify_regress(titanic_lite.iloc[i,:], titanic_tree)\n",
    "    titanic_prediction.append(obs_pred)\n",
    "\n",
    "print(\"Predictions: \", titanic_prediction,\n",
    "      \"\\n\\n Real values:\", titanic_lite.Survived[:num_obs].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(titanic_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada67155",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tree['Fare <=  52.5542'][1]#['Fare <=  10.5'][0]['Age <=  33.0'][0][\"Embarked in  ['S']\"][0]['Age <=  31.0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbf40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = titanic_tree['Fare <=  52.5542']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ebf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab2d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145263e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024fe11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = v1[0]['Fare <=  10.5']\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v22 = v1[1]['Age <=  64.0']\n",
    "v22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478de99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f94fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v3 = v2[0]['Age <=  33.0']\n",
    "v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "v3[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4 = v3[0][\"Embarked in  ['S']\"]\n",
    "v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484952e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da09068",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5 = v4[0]['Age <=  31.0']\n",
    "v5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d91a1",
   "metadata": {},
   "source": [
    "<h3> random forest </h3> \n",
    "<h4> method to get a bootstrap of from dataset </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bootstrap(x):\n",
    "    \"\"\"_gets a bootstrap of all the samples\n",
    "    \n",
    "    Args:\n",
    "        x (_Pandas DataFrame_): _training dataset with labels_\n",
    "    \n",
    "    Return:\n",
    "        _x_: a bootstrapped dataset\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return x.iloc[idxs, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(data, y, is_numeric, n_trees=100):\n",
    "    \"\"\"_train a random fores_\n",
    "\n",
    "    Args:\n",
    "        data (_Pandas DataFrame_): _training data for the Decision Tree_\n",
    "        y (_string_): _name of target varible_\n",
    "        is_numeric (_bool_): is target variable categorical or numeric\n",
    "        n_trees (_int_): number of trees in forest\n",
    "        \n",
    "    Returns:\n",
    "        _trees_: _list of decision trees trained_\n",
    "    \"\"\"    \n",
    "    trees = [] # list of trees \n",
    "    for _ in range(n_trees):\n",
    "        x_samples = draw_bootstrap(data)\n",
    "        tree = train_tree(data,y, is_numeric, max_depth = None,min_samples_split = None, min_information_gain = 1e-20, counter=0, max_categories = 20)\n",
    "        trees.append(tree)\n",
    "    return trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f24679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(trees, is_numeric, X):\n",
    "    \"\"\"finds the most common label in the forest\n",
    "    \n",
    "    Args:\n",
    "        trees (_list_): a list of trained trees (forest)\n",
    "        is_numeric (_bool_): classifier or regressor\n",
    "        X (_Pandas DataFrame_): training data without the label\n",
    "        \n",
    "    Return:\n",
    "        most_common (_int/float_): the most common answer\n",
    "    \"\"\"\n",
    "    if is_numeric:\n",
    "        pass\n",
    "    else:\n",
    "        num_obs = X.shape(0)\n",
    "        predictions = []\n",
    "        for tree in trees:\n",
    "            tree_pred = [classify_regress(X.iloc[i,:], tree) for i in range(num_obs)]\n",
    "            predictions.append(tree_pred)\n",
    "    predictions = np.array(predictions)\n",
    "    most_common = np.round(np.mean(predictions, axis=0), 0)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17beeeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b027b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc84357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdfbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e2506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5daa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "am = np.random.choice(15, 15, replace=True)\n",
    "k.iloc[am, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.iloc[am]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = draw_bootstrap(x=k, y =j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = titanic.head(15)\n",
    "j = titanic.head(15)\n",
    "k = k.loc[:, ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age']]\n",
    "j = j.loc[:, 'Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adac531",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aca68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6a5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
